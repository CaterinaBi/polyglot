# multilingual
This repository includes various multilingual related studies conducted by EleutherAI. Starting with GPT-3, researches about large-scale language model has been advanced so far, but most of it has been done in English. But we wanted to make sure that non-English-speaking countries can also benefit from these researches because  there are many languages in the world.

Our ultimate goal is making language models that can learn languages other than English well. To this end, we plan to perform various language-related experiments such as multilingual language transferring, non-English monolingual training and multilingual training, and we will record them here.


## 1. Korean monolingual models

## 2. Coming Soon...

## 3. Citation
